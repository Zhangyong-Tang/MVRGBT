# MV-RGBT

‚≠ê This main contributions are:

 - [x] A new benchmark, MV-RGBT, and a new way for in-depth analysis of the methods.

 - [x] A new problem, when to fuse, is introduced to explore the necessity of multi-modal fusion.

 - [x] A new method, MoETrack, is derived with a mixture of experts.

 - [x] New state-of-the-art results on several benchmarks, including MV-RGBT, RGBT234, LasHeR, and VTUAV-ST.
 
 - [x] The code will be available [here](https://github.com/Zhangyong-Tang/MoETrack)

ü´µFind our survey work at another [repo](https://github.com/Zhangyong-Tang/Survey-for-MultiModal-Visual-Object-Tracking)

## Benchmark Data Comparison
### ‚≠ê MV-RGBT will be published after this work accepted!

<img src="ER_Cat_Lawn0.gif" width="800">

<img src="ET_Fish_River02.gif" width="800">


<img src="figs/data.png" width="600">

## Selection Results

LasHeR:
---
<img src="figs/results-LasHeR.png" width="600">

MV-RGBT:
---
<img src="figs/results-MV-RGBT.png" width="600">

