# Motivation

![8ceb2c866b3b579fffd9f52826f6859](https://github.com/user-attachments/assets/db23fed8-8052-4a77-a32a-cef4b4e9fb3d)

# üç∞Contributions
(1) A new benchmark, MV-RGBT, is collected to make it representative of multi-modal warranting scenarios, filling the gap between the data in current benchmarks and imaging conditions which motivate RGBT tracking.

(2) A new problem, `when to fuse', is posed to develop reliable fusion strategies for RGBT trackers, as in MMW scenarios multi-modal information fusion may be counterproductive. To facilitate its discussion, a new solution, MoETrack, with multiple tracking experts is proposed. It performs state-of-the-art on several benchmarks, including MV-RGBT, LasHeR, and VTUAV-ST.

(3) A new compositional perspective for method evaluation is provided by categorising MV-RGBT into two subsets, MV-RGBT-RGB and MV-RGBT-TIR, promoting a novel in-depth analysis and offering insightful recommendations for future developments in RGBT tracking.

ü´µFind our survey work at [repo](https://github.com/Zhangyong-Tang/Survey-for-MultiModal-Visual-Object-Tracking)

## Benchmark Data Comparison
### ‚≠ê MV-RGBT will be published after this work accepted!

<img src="ER_Cat_Lawn0.gif" width="800">

<img src="ET_Fish_River02.gif" width="800">


<img src="figs/data.png" width="600">

## Selection Results

LasHeR:
---
<img src="figs/results-LasHeR.png" width="600">

MV-RGBT:
---
<img src="figs/results-MV-RGBT.png" width="600">

‚≠ê More detailed introduction of the proposed method, MoETrack, is available [here](https://github.com/Zhangyong-Tang/MoETrack)

